# Experiments on french to english translation

Migration of OpenNMT from Torch (obsolete) to PyTorch is necessary. A lot of things changed:

1. Parameters are not the same. epoch/steps removed, now.
1. NN snapshots can't be transfered, because old ones need Torch/Cuda to convert to no GPU basic snapshot but Cuda was upgraded... Conversion is no more possible.

Seed 0 is now used everywere possible to reproduce exactly (?) all experiments.

## Scripts
`preprocess.sh` to preprocess data (chunkify, etc)

`training.sh` to train NN.

`translate.sh` to translate data.

## How to set parameters

OpenNMT Torch used epoch and such; now how to set parameters to mimic old behavior?
Note: Actually, I don't know why these options disappeared.

Learning process uses loops (training loops).

The network is feed with `batch_size` sentences and the model is updated.

`train_steps` (or iterations) is the number of loops to compute before updating the model (backward propagation).

An `epoch` corresponds to the fact that the dataset has passed entirely through the training AND at least a back propagation occured (model update).

Parameters (example) :

2000000 sentences, `batch_size` 100, epoch 20 ==> `train_steps` = `epoch`*sentences/`batch_size`

If you need to get intermediate models (for example one for each epoch) :
`save_checkpoint_steps` = sentences/`batch_size`

Note: the greater is **batch_size**, the more memory is used...

## Patching Translation module to mimic `-replace_unk_tagged`
Missing `-replace_unk_tagged` option. Lot of users complains about it.
Patch added to `Translation.py` in installed distrib.

File : `/usr/local/lib/python3.7/dist-packages/OpenNMT_py-1.0.0rc1-py3.7.egg/onmt/translate/translation.py`

Original renamed `translation.py.orig`

Diff is :

    56a57
    >                     tokens[i] = "{{"+tokens[i]+"}}"
    
