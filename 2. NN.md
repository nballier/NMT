*Aide-mémoire pour les problématiques des RN pour la TAN*




### *** le prétraitement**
Pour limiter le vocabulaire , la stratégie est un découpage en sous-mots qui s'éloigne des modèles de chaînes de caractère. Le paramétrage de l'outil semble varier selon les articles. [Servan et al. 2017](taln2017.cnrs.fr/wp-content/uploads/2017/06/actes_TALN_2017-vol2Final.pdf#page=230)

"Les données sont pré-traitées en utilisant une méthode de découpage en sous-mots issue de l’al- gorithme de compression de données : « byte pair encoding »(Sennrich et al., 2016b) avec 30 000 opérations. Nous avons conservé les 32 000 mots plus fréquents pour les langues source et cible."
Servan, Chr., Crego, J and Senellart, J. (2017) Adaptation incrémentale de modèles de traduction neuronaux. In I. Eshkol and J.-Y. Antoine (Eds.) 24e Conférence sur le Traitement Automatique des Langues Naturelles TALN2017.  218-226.




### `batch_size` ?
[À propos des paramètres d'OpenNMT](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/). Le site, en général, contient de bonnes informations sur la façon dont fonctionne le deep learning. Du coup, normalement il faudrait que le nombre d'échantillons soit un mutiple du `batch_size`, sinon il pourrait y avoir un biais (est-ce vrai ?). Une façon de s'en sortir pourrait être de sortir des échantillons de l'entraînement et de les utiliser comme échantillons de validation!

L'article [ici](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) est explicite, il faut (en général) prendre les valeurs les plus petites possibles! Mais j'ai l'impression que c'est au prix d'un calcul plus long... Il faudra tester!

### `Differences between the gradient descent optimization algorithms`
https://ruder.io/optimizing-gradient-descent/
