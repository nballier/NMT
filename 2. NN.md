### `batch_size` ?
[À propos des paramètres d'OpenNMT](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/). Le site, en général, contient de bonnes informations sur la façon dont fonctionne le deep learning. Du coup, normalement il faudrait que le nombre d'échantillons soit un mutiple du `batch_size`, sinon il pourrait y avoir un biais (est-ce vrai ?). Une façon de s'en sortir pourrait être de sortir des échantillons de l'entraînement et de les utiliser comme échantillons de validation!

L'article [ici](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) est explicite, il faut (en général) prendre les valeurs les plus petites possibles! Mais j'ai l'impression que c'est au prix d'un calcul plus long... Il faudra tester!
