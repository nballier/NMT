*Aide-mémoire pour les problématiques des RN pour la TAN*




### *** le prétraitement**
Pour limiter le vocabulaire , la stratégie est un découpage en sous-mots qui s'éloigne des modèles de chaînes de caractère. Le paramétrage de l'outil semble varier selon les articles. [Servan et al. 2017](taln2017.cnrs.fr/wp-content/uploads/2017/06/actes_TALN_2017-vol2Final.pdf#page=230)

"Les données sont pré-traitées en utilisant une méthode de découpage en sous-mots issue de l’al- gorithme de compression de données : « byte pair encoding »(Sennrich et al., 2016b) avec 30 000 opérations. Nous avons conservé les 32 000 mots plus fréquents pour les langues source et cible."
Servan, Chr., Crego, J and Senellart, J. (2017) Adaptation incrémentale de modèles de traduction neuronaux. In I. Eshkol and J.-Y. Antoine (Eds.) 24e Conférence sur le Traitement Automatique des Langues Naturelles TALN2017.  218-226.

De manière amusante, le système produit des résultats pour le français, très insatisfaisants et irréguliers. Des outils de reconnaissance morphologique non supervsiée type Linguistica ou chipmunk donneraient sans doute des résultats moins aberrants.

Lien pour l'outil de tagging BPE : https://github.com/rsennrich/subword-nmt


### le gold score 

"Gold score is the log likelihood of the reference that you provided during translation." G. Klein
cf. : http://forum.opennmt.net/t/what-exactly-is-the-gold-score/2920/2


### le beam search 
[papier de référence](https://www.aclweb.org/anthology/W17-3207/)
"The original beam-search strategy finds a translation that approximately maximizes the conditional probability given by a specific model." (Freitag & Al-Onaizan, 2017) 
"Beam search decoding only improves translation quality for narrow beams and deteriorates when exposed to a larger search space."  (Koehn and Knowkes 2017, Six Challenges for Neural Machine Translation)

[coder un beam search en python](https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/)



### `batch_size` ?
[À propos des paramètres d'OpenNMT](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/). Le site, en général, contient de bonnes informations sur la façon dont fonctionne le deep learning. Du coup, normalement il faudrait que le nombre d'échantillons soit un mutiple du `batch_size`, sinon il pourrait y avoir un biais (est-ce vrai ?). Une façon de s'en sortir pourrait être de sortir des échantillons de l'entraînement et de les utiliser comme échantillons de validation!

L'article [ici](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) est explicite, il faut (en général) prendre les valeurs les plus petites possibles! Mais j'ai l'impression que c'est au prix d'un calcul plus long... Il faudra tester!

### `Differences between the gradient descent optimization algorithms`
https://ruder.io/optimizing-gradient-descent/

### `Neural Machine Translation and Sequence-to-sequence Models: A Tutorial`

https://arxiv.org/pdf/1703.01619.pdf

### `NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE`
https://arxiv.org/pdf/1409.0473.pdf

### `Selecting Artificially-Generated Sentences for Fine-Tuning Neural Machine Translation`
https://arxiv.org/pdf/1909.12016.pdf

### `On the Importance of Word Boundaries in Character-level Neural Machine Translation`
https://arxiv.org/pdf/1910.06753.pdf
